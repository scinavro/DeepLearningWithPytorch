{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[소프트맥스 회귀의 비용 함수 구현하기]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2de88807fd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim = 0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],\n",
      "        [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],\n",
      "        [0.6387, 0.5247, 0.6826, 0.3051, 0.4635]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad = True)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim = 1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "print(y_one_hot)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3301, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4602, 0.0000, 0.0000],\n",
      "        [0.0000, 1.6165, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "tensor([1.3301, 1.4602, 1.6165], grad_fn=<SumBackward1>)\n",
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim = 1).mean()\n",
    "print((y_one_hot * -torch.log(hypothesis)))\n",
    "print((y_one_hot * -torch.log(hypothesis)).sum(dim = 1))\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
      "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
      "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)\n",
      "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
      "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
      "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.log(F.softmax(z, dim = 1)))\n",
    "print(F.log_softmax(z, dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n",
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n",
      "tensor(1.4689, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print((y_one_hot * -torch.log(F.softmax(z, dim = 1))).sum(dim = 1).mean())\n",
    "print((y_one_hot * -F.log_softmax(z, dim = 1)).sum(dim = 1).mean())\n",
    "print(F.cross_entropy(z, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[소프트맥스 회귀 구현하기]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2de88807fd0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n",
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(8, 3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot)\n",
    "print(y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((4, 3), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim = 1)\n",
    "    \n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim = 1).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.6761, -0.1014,  3.7776],\n",
      "        [-0.6601, -0.4641,  1.1242],\n",
      "        [ 2.3178, -0.4808, -1.8370],\n",
      "        [-0.6030,  1.4281, -0.8252]], requires_grad=True)\n",
      "tensor([6.1765e-07], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.704199\n",
      "Epoch  200/1000 Cost: 0.623000\n",
      "Epoch  300/1000 Cost: 0.565717\n",
      "Epoch  400/1000 Cost: 0.515291\n",
      "Epoch  500/1000 Cost: 0.467662\n",
      "Epoch  600/1000 Cost: 0.421278\n",
      "Epoch  700/1000 Cost: 0.375402\n",
      "Epoch  800/1000 Cost: 0.329766\n",
      "Epoch  900/1000 Cost: 0.285073\n",
      "Epoch 1000/1000 Cost: 0.248155\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((4, 3), requires_grad = True)\n",
    "b = torch.zeros(3, requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0408,  0.4448,  2.5961],\n",
      "        [-0.3537, -0.1772,  0.5308],\n",
      "        [ 2.3953, -0.4589, -1.9364],\n",
      "        [-0.6828,  1.0930, -0.4102]], requires_grad=True)\n",
      "tensor([-2.3578, -0.9655,  3.3234], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.616785\n",
      "Epoch  100/1000 Cost: 0.658891\n",
      "Epoch  200/1000 Cost: 0.573443\n",
      "Epoch  300/1000 Cost: 0.518151\n",
      "Epoch  400/1000 Cost: 0.473265\n",
      "Epoch  500/1000 Cost: 0.433516\n",
      "Epoch  600/1000 Cost: 0.396563\n",
      "Epoch  700/1000 Cost: 0.360914\n",
      "Epoch  800/1000 Cost: 0.325392\n",
      "Epoch  900/1000 Cost: 0.289178\n",
      "Epoch 1000/1000 Cost: 0.254148\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-3.1120, -0.3276,  2.3522, -0.4711],\n",
      "        [ 0.4027, -0.1933, -0.4841,  1.2384],\n",
      "        [ 2.5657,  0.5388, -1.9292, -0.2536]], requires_grad=True), Parameter containing:\n",
      "tensor([-2.2920, -1.0112,  3.2545], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())) #Bias 가 Class 개수에 맞춰서 3개가 생성된다!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 2.637636\n",
      "Epoch  100/1000 Cost: 0.647903\n",
      "Epoch  200/1000 Cost: 0.564643\n",
      "Epoch  300/1000 Cost: 0.511043\n",
      "Epoch  400/1000 Cost: 0.467249\n",
      "Epoch  500/1000 Cost: 0.428281\n",
      "Epoch  600/1000 Cost: 0.391924\n",
      "Epoch  700/1000 Cost: 0.356742\n",
      "Epoch  800/1000 Cost: 0.321577\n",
      "Epoch  900/1000 Cost: 0.285617\n",
      "Epoch 1000/1000 Cost: 0.250818\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[소프트맥스 회귀로 MNIST 데이터 분류하기]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(777)\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/', train = True, transform = transforms.ToTensor(), download = True)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', train = False, transform = transforms.ToTensor(), download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000002DE92A5DC70>\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(dataset = mnist_train, batch_size = batch_size, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(784, 10, bias = True).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost =  0.273159742\n",
      "Epoch: 0002 cost =  0.271731675\n",
      "Epoch: 0003 cost =  0.270446241\n",
      "Epoch: 0004 cost =  0.269391030\n",
      "Epoch: 0005 cost =  0.268237293\n",
      "Epoch: 0006 cost =  0.267374694\n",
      "Epoch: 0007 cost =  0.266501278\n",
      "Epoch: 0008 cost =  0.265565038\n",
      "Epoch: 0009 cost =  0.265064389\n",
      "Epoch: 0010 cost =  0.264051646\n",
      "Epoch: 0011 cost =  0.263369322\n",
      "Epoch: 0012 cost =  0.262597471\n",
      "Epoch: 0013 cost =  0.261987746\n",
      "Epoch: 0014 cost =  0.261226773\n",
      "Epoch: 0015 cost =  0.260715187\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8758999705314636\n",
      "Label:  2\n",
      "Prediction:  7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    \n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "    \n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap = 'Greys', interpolation = 'nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 784])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 10])\n",
      "tensor([[  194.8162, -2482.9348,   336.8049,  ...,  2545.8313,   432.2655,\n",
      "           798.8593],\n",
      "        [ 1336.8130,  -270.5056,  2681.6221,  ..., -4181.1138,  1545.8136,\n",
      "         -2188.7104],\n",
      "        [-1390.7625,  1507.7030,   452.0786,  ...,   -26.5968,   569.2531,\n",
      "          -299.0349],\n",
      "        ...,\n",
      "        [-1887.8983, -1892.8247,  -645.3472,  ...,   347.7263,  1395.5918,\n",
      "          1184.3586],\n",
      "        [ -477.8851,  -442.7889,  -479.6142,  ..., -1346.5920,  1689.6226,\n",
      "          -678.8488],\n",
      "        [  999.5863, -2669.9946,  1376.9954,  ..., -2106.2266,   458.2175,\n",
      "          -934.2053]])\n",
      "tensor([7, 2, 1,  ..., 4, 8, 6])\n",
      "tensor([7, 2, 1,  ..., 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(prediction.shape)\n",
    "print(prediction)\n",
    "print(torch.argmax(prediction, 1))\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
